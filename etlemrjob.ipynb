{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col, trim, to_date, when\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1a0e7969-58cc-49b8-abfa-178a38ca5cfe.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1a0e7969-58cc-49b8-abfa-178a38ca5cfe.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1a0e7969-58cc-49b8-abfa-178a38ca5cfe.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1a0e7969-58cc-49b8-abfa-178a38ca5cfe.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Loading the dataset into a DynamicFrame\ndynamicFrameEnigma = glueContext.create_dynamic_frame.from_catalog(database=\"coviddatabase\", table_name=\"enigma-jhu\")\n\n# Since, the above python code will show col0, col1,...as first row. so by mapping the column names to new ones\nmapped_dynamic_frame = dynamicFrameEngima.apply_mapping([\n    ('col0', 'string', 'fips', 'string'),\n    ('col1', 'string', 'admin2', 'string'),\n    ('col2', 'string', 'province_state', 'string'),\n    ('col3', 'string', 'country_region', 'string'),\n    ('col4', 'string', 'last_update', 'string'),\n    ('col5', 'string', 'latitude', 'string'),\n    ('col6', 'string', 'longitude', 'string'),\n    ('col7', 'string', 'confirmed', 'string'),\n    ('col8', 'string', 'deaths', 'string'),\n    ('col9', 'string', 'recovered', 'string'),\n    ('col10', 'string', 'active', 'string'),\n    ('col11', 'string', 'combined_key', 'string')\n])\n\n# Converting the DynamicFrame to an iterable of records\nrecords = mapped_dynamic_frame.toDF().collect()\n\n# Shifting the rows (make the second row the first row)\nheader = records.pop(0)\n\n# Creating a new DynamicFrame from the remaining records\nnewdynamicFrameEnigma = DynamicFrame.fromDF(spark.createDataFrame(records, schema=header), glueContext, \"newdynamicFrameEnigma\")\n\n# Showing Dynamic Frame to Spark DataFrame\nsparkDf = newdynamicFrameEnigma.toDF()\n# Showing spark DF\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\n|fips|admin2|province_state|country_region|        last_update|latitude|longitude|confirmed|deaths|recovered|active|    combined_key|\n+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\n|    |      |         Anhui|         China|2020-01-22T17:00:00|  31.826|  117.226|        1|      |         |      |    Anhui, China|\n|    |      |       Beijing|         China|2020-01-22T17:00:00|  40.182|  116.414|       14|      |         |      |  Beijing, China|\n|    |      |     Chongqing|         China|2020-01-22T17:00:00|  30.057|  107.874|        6|      |         |      |Chongqing, China|\n|    |      |        Fujian|         China|2020-01-22T17:00:00|  26.079|  117.987|        1|      |         |      |   Fujian, China|\n|    |      |         Gansu|         China|2020-01-22T17:00:00|  36.061|  103.834|         |      |         |      |    Gansu, China|\n|    |      |     Guangdong|         China|2020-01-22T17:00:00|  23.342|  113.424|       26|      |         |      |Guangdong, China|\n|    |      |       Guangxi|         China|2020-01-22T17:00:00|   23.83|  108.788|        2|      |         |      |  Guangxi, China|\n|    |      |       Guizhou|         China|2020-01-22T17:00:00|  26.815|  106.875|        1|      |         |      |  Guizhou, China|\n|    |      |           Hai|         China|2020-01-22T17:00:00|  19.196|  109.745|        4|      |         |      |      Hai, China|\n|    |      |         Hebei|         China|2020-01-22T17:00:00|  38.043|  114.515|        1|      |         |      |    Hebei, China|\n+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Data Cleaning \n# Handling Missing and Null Values\n# Replacing empty strings with None to handle them as null values\nsparkDf = sparkDf.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in sparkDf.columns])\n\n# Replacing missing and null values with default values\nfill_values = {\n    \"fips\": \"Unknown\",\n    \"admin2\": \"Unknown\",\n    \"province_state\": \"Unknown\",\n    \"country_region\": \"Unknown\",\n    \"latitude\": 0.0,\n    \"longitude\": 0.0,\n    \"confirmed\": 0,\n    \"deaths\": 0,\n    \"recovered\": 0,\n    \"active\": 0,\n    \"combined_key\": \"Unknown\"\n}\n\n# Applying fill values for nulls\nsparkDf = sparkDf.na.fill(fill_values)\n\n# Show the DataFrame to inspect the first 10 rows after filling values\nsparkDf.show(10)\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------+-------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\n|   fips| admin2|province_state|country_region|        last_update|latitude|longitude|confirmed|deaths|recovered|active|    combined_key|\n+-------+-------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\n|Unknown|Unknown|         Anhui|         China|2020-01-22T17:00:00|  31.826|  117.226|        1|     0|        0|     0|    Anhui, China|\n|Unknown|Unknown|       Beijing|         China|2020-01-22T17:00:00|  40.182|  116.414|       14|     0|        0|     0|  Beijing, China|\n|Unknown|Unknown|     Chongqing|         China|2020-01-22T17:00:00|  30.057|  107.874|        6|     0|        0|     0|Chongqing, China|\n|Unknown|Unknown|        Fujian|         China|2020-01-22T17:00:00|  26.079|  117.987|        1|     0|        0|     0|   Fujian, China|\n|Unknown|Unknown|         Gansu|         China|2020-01-22T17:00:00|  36.061|  103.834|        0|     0|        0|     0|    Gansu, China|\n|Unknown|Unknown|     Guangdong|         China|2020-01-22T17:00:00|  23.342|  113.424|       26|     0|        0|     0|Guangdong, China|\n|Unknown|Unknown|       Guangxi|         China|2020-01-22T17:00:00|   23.83|  108.788|        2|     0|        0|     0|  Guangxi, China|\n|Unknown|Unknown|       Guizhou|         China|2020-01-22T17:00:00|  26.815|  106.875|        1|     0|        0|     0|  Guizhou, China|\n|Unknown|Unknown|           Hai|         China|2020-01-22T17:00:00|  19.196|  109.745|        4|     0|        0|     0|      Hai, China|\n|Unknown|Unknown|         Hebei|         China|2020-01-22T17:00:00|  38.043|  114.515|        1|     0|        0|     0|    Hebei, China|\n+-------+-------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+----------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting back to DynamicFrame for writing to S3\ncleaned_dynamic_frame = DynamicFrame.fromDF(sparkDf, glueContext, \"cleaned_dynamic_frame\")\n\n# Write the cleaned data back to S3\noutput_path = \"s3://etlemrbucket/cleandata/\"\nglueContext.write_dynamic_frame.from_options(\n    frame=cleaned_dynamic_frame,\n    connection_type=\"s3\",\n    connection_options={\"path\": output_path},\n    format=\"csv\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f83b6ddaaa0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Loading the dataset into a DynamicFrame\ndynamicFrameUSstates = glueContext.create_dynamic_frame.from_catalog(database=\"coviddatabase\", table_name=\"us_states\")\n# Map the column names to new ones\nmapped_dynamic_frame = dynamicFrameUSstates.apply_mapping([\n    ('col0', 'string', 'date', 'string'),\n    ('col1', 'string', 'state', 'string'),\n    ('col2', 'string', 'fips', 'string'),\n    ('col3', 'string', 'cases', 'string'),\n    ('col4', 'string', 'deaths', 'string')\n])\n# Converting the DynamicFrame to an iterable of records\nrecords = mapped_dynamic_frame.toDF().collect()\n\n# Shifting the rows (make the second row the first row)\nheader = records.pop(0)\n\n# Creating a new DynamicFrame from the remaining records\nnewdynamicFrameUSstates = DynamicFrame.fromDF(spark.createDataFrame(records, schema=header), glueContext, \"newdynamicFrameUSstates\")\n\n# Showing Dynamic Frame to Spark DataFrame\nsparkDf = newdynamicFrameUSstates.toDF()\n# Showing spark DF\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+----+-----+------+\n|      date|     state|fips|cases|deaths|\n+----------+----------+----+-----+------+\n|2020-01-21|Washington|  53|    1|     0|\n|2020-01-22|Washington|  53|    1|     0|\n|2020-01-23|Washington|  53|    1|     0|\n|2020-01-24|  Illinois|  17|    1|     0|\n|2020-01-24|Washington|  53|    1|     0|\n|2020-01-25|California|  06|    1|     0|\n|2020-01-25|  Illinois|  17|    1|     0|\n|2020-01-25|Washington|  53|    1|     0|\n|2020-01-26|   Arizona|  04|    1|     0|\n|2020-01-26|California|  06|    2|     0|\n+----------+----------+----+-----+------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Handling Missing and Null Values\n# Replacing empty strings with None to handle them as null values\nsparkDf = sparkDf.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in sparkDf.columns])\n\n# Replacing missing and null values with default values\nfill_values = {\n    \"date\": \"1970-01-01\",\n    \"state\": \"Unknown\",\n    \"fips\": \"000\",\n    \"cases\": 0,\n    \"deaths\": 0\n}\n\n# Applying fill values for nulls\nsparkDf = sparkDf.na.fill(fill_values)\n# Showing the DataFrame to inspect the first 10 rows after filling values\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+----+-----+------+\n|      date|     state|fips|cases|deaths|\n+----------+----------+----+-----+------+\n|2020-01-21|Washington|  53|    1|     0|\n|2020-01-22|Washington|  53|    1|     0|\n|2020-01-23|Washington|  53|    1|     0|\n|2020-01-24|  Illinois|  17|    1|     0|\n|2020-01-24|Washington|  53|    1|     0|\n|2020-01-25|California|  06|    1|     0|\n|2020-01-25|  Illinois|  17|    1|     0|\n|2020-01-25|Washington|  53|    1|     0|\n|2020-01-26|   Arizona|  04|    1|     0|\n|2020-01-26|California|  06|    2|     0|\n+----------+----------+----+-----+------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting back to DynamicFrame for writing to S3\ncleaned_dynamic_frame = DynamicFrame.fromDF(sparkDf, glueContext, \"cleaned_dynamic_frame\")\n\n# Write the cleaned data back to S3\noutput_path = \"s3://etlemrbucket/cleandata/\"\nglueContext.write_dynamic_frame.from_options(\n    frame=cleaned_dynamic_frame,\n    connection_type=\"s3\",\n    connection_options={\"path\": output_path},\n    format=\"csv\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f83d62e13c0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Loading the dataset into a DynamicFrame\ndynamicFrameUScounties = glueContext.create_dynamic_frame.from_catalog(database=\"coviddatabase\", table_name=\"us_counties\")\n# Map the column names to new ones\nmapped_dynamic_frame = dynamicFrameUScounties.apply_mapping([\n    ('col0', 'string', 'date', 'string'),\n    ('col1', 'string', 'county', 'string'),\n    ('col2', 'string', 'state', 'string'),\n    ('col3', 'string', 'fips', 'string'),\n    ('col4', 'string', 'cases', 'string'),\n    ('col5', 'string', 'deaths', 'string')\n])\n# Converting the DynamicFrame to an iterable of records\nrecords = mapped_dynamic_frame.toDF().collect()\n\n# Shifting the rows (make the second row the first row)\nheader = records.pop(0)\n\n# Creating a new DynamicFrame from the remaining records\nnewdynamicFrameUScounties = DynamicFrame.fromDF(spark.createDataFrame(records, schema=header), glueContext, \"newdynamicFrameUScounties\")\n\n# Showing Dynamic Frame to Spark DataFrame\nsparkDf = newdynamicFrameUScounties.toDF()\n# Showing spark DF\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----------+----------+-----+-----+------+\n|      date|     county|     state| fips|cases|deaths|\n+----------+-----------+----------+-----+-----+------+\n|2020-01-21|  Snohomish|Washington|53061|    1|     0|\n|2020-01-22|  Snohomish|Washington|53061|    1|     0|\n|2020-01-23|  Snohomish|Washington|53061|    1|     0|\n|2020-01-24|       Cook|  Illinois|17031|    1|     0|\n|2020-01-24|  Snohomish|Washington|53061|    1|     0|\n|2020-01-25|     Orange|California|06059|    1|     0|\n|2020-01-25|       Cook|  Illinois|17031|    1|     0|\n|2020-01-25|  Snohomish|Washington|53061|    1|     0|\n|2020-01-26|   Maricopa|   Arizona|04013|    1|     0|\n|2020-01-26|Los Angeles|California|06037|    1|     0|\n+----------+-----------+----------+-----+-----+------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Handle Missing and Null Values\n# Replace empty strings with None to handle them as null values\nsparkDf = sparkDf.select([when(col(c) == \"\", None).otherwise(col(c)).alias(c) for c in sparkDf.columns])\n\n# Define fill values for nulls\nfill_values = {\n    \"date\": \"1970-01-01\",\n    \"county\": \"Unknown\",\n    \"state\": \"Unknown\",\n    \"fips\": \"000\",\n    \"cases\": 0,\n    \"deaths\": 0\n}\n\n# Apply fill values for nulls\nsparkDf = sparkDf.na.fill(fill_values)\n# Showing the DataFrame to inspect the first 10 rows after filling values\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----------+----------+-----+-----+------+\n|      date|     county|     state| fips|cases|deaths|\n+----------+-----------+----------+-----+-----+------+\n|2020-01-21|  Snohomish|Washington|53061|    1|     0|\n|2020-01-22|  Snohomish|Washington|53061|    1|     0|\n|2020-01-23|  Snohomish|Washington|53061|    1|     0|\n|2020-01-24|       Cook|  Illinois|17031|    1|     0|\n|2020-01-24|  Snohomish|Washington|53061|    1|     0|\n|2020-01-25|     Orange|California|06059|    1|     0|\n|2020-01-25|       Cook|  Illinois|17031|    1|     0|\n|2020-01-25|  Snohomish|Washington|53061|    1|     0|\n|2020-01-26|   Maricopa|   Arizona|04013|    1|     0|\n|2020-01-26|Los Angeles|California|06037|    1|     0|\n+----------+-----------+----------+-----+-----+------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting back to DynamicFrame for writing to S3\ncleaned_dynamic_frame = DynamicFrame.fromDF(sparkDf, glueContext, \"cleaned_dynamic_frame\")\n\n# Write the cleaned data back to S3\noutput_path = \"s3://etlemrbucket/cleandata/\"\nglueContext.write_dynamic_frame.from_options(\n    frame=cleaned_dynamic_frame,\n    connection_type=\"s3\",\n    connection_options={\"path\": output_path},\n    format=\"csv\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f83c27c6b00>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Loading the dataset into a DynamicFrame\ndynamicFrameUSdaily = glueContext.create_dynamic_frame.from_catalog(database=\"coviddatabase\", table_name=\"us_daily\")\n\n# Since, the above python code will show col0, col1,...as first row. so by mapping the column names to new ones\n# Map the column names to new ones\nmapped_dynamic_frame = dynamicFrameUSdaily.apply_mapping([\n    ('col0', 'string', 'date', 'string'),\n    ('col1', 'string', 'states', 'string'),\n    ('col2', 'string', 'positive', 'string'),\n    ('col3', 'string', 'negative', 'string'),\n    ('col4', 'string', 'pending', 'string'),\n    ('col5', 'string', 'hospitalizedCurrently', 'string'),\n    ('col6', 'string', 'hospitalizedCumulative', 'string'),\n    ('col7', 'string', 'inIcuCurrently', 'string'),\n    ('col8', 'string', 'inIcuCumulative', 'string'),\n    ('col9', 'string', 'onVentilatorCurrently', 'string'),\n    ('col10', 'string', 'onVentilatorCumulative', 'string'),\n    ('col11', 'string', 'dateChecked', 'string'),\n    ('col12', 'string', 'death', 'string'),\n    ('col13', 'string', 'hospitalized', 'string'),\n    ('col14', 'string', 'totalTestResults', 'string'),\n    ('col15', 'string', 'lastModified', 'string'),\n    ('col16', 'string', 'recovered', 'string'),\n    ('col17', 'string', 'total', 'string'),\n    ('col18', 'string', 'posNeg', 'string'),\n    ('col19', 'string', 'deathIncrease', 'string'),\n    ('col20', 'string', 'hospitalizedIncrease', 'string'),\n    ('col21', 'string', 'negativeIncrease', 'string'),\n    ('col22', 'string', 'positiveIncrease', 'string'),\n    ('col23', 'string', 'totalTestResultsIncrease', 'string'),\n    ('col24', 'string', 'hash', 'string')\n])\n\n# Converting the DynamicFrame to an iterable of records\nrecords = mapped_dynamic_frame.toDF().collect()\n\n# Shifting the rows (make the second row the first row)\nheader = records.pop(0)\n\n# Creating a new DynamicFrame from the remaining records\nnewdynamicFrameUSdaily = DynamicFrame.fromDF(spark.createDataFrame(records, schema=header), glueContext, \"newdynamicFrameUSdaily\")\n\n# Showing Dynamic Frame to Spark DataFrame\nsparkDf = newdynamicFrameUSdaily.toDF()\n# Showing spark DF\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\n|    date|states|positive|negative|pending|hospitalizedCurrently|hospitalizedCumulative|inIcuCurrently|inIcuCumulative|onVentilatorCurrently|onVentilatorCumulative|         dateChecked| death|hospitalized|totalTestResults|        lastModified|recovered|total|posNeg|deathIncrease|hospitalizedIncrease|negativeIncrease|positiveIncrease|totalTestResultsIncrease|                hash|\n+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\n|20210307|    56|28755524|74579770|  11808|                40212|                878613|          8137|          45475|                 2801|                  4281|2021-03-07T24:00:00Z|515142|      878613|       363789451|2021-03-07T24:00:00Z|         |    0|     0|          839|                 726|          130414|           41265|                 1156241|8b26839690cd05c0c...|\n|20210306|    56|28714259|74449356|  11783|                41401|                877887|          8409|          45453|                 2811|                  4280|2021-03-06T24:00:00Z|514303|      877887|       362633210|2021-03-06T24:00:00Z|         |    0|     0|         1674|                 503|          142201|           59620|                 1409138|d0c0482ea549c9d5c...|\n|20210305|    56|28654639|74307155|  12213|                42541|                877384|          8634|          45373|                 2889|                  4275|2021-03-05T24:00:00Z|512629|      877384|       361224072|2021-03-05T24:00:00Z|         |    0|     0|         2221|                2781|          271917|           68787|                 1744417|a35ea4289cec4bb55...|\n|20210304|    56|28585852|74035238|  12405|                44172|                874603|          8970|          45293|                 2973|                  4267|2021-03-04T24:00:00Z|510408|      874603|       359479655|2021-03-04T24:00:00Z|         |    0|     0|         1743|                1530|          177957|           65487|                 1590984|a19ad6379a653834c...|\n|20210303|    56|28520365|73857281|  11778|                45462|                873073|          9359|          45214|                 3094|                  4260|2021-03-03T24:00:00Z|508665|      873073|       357888671|2021-03-03T24:00:00Z|         |    0|     0|         2449|                2172|          267001|           66836|                 1406795|9e1d2afda1b0ec243...|\n|20210302|    56|28453529|73590280|  11196|                46388|                870901|          9465|          45084|                 3169|                  4257|2021-03-02T24:00:00Z|506216|      870901|       356481876|2021-03-02T24:00:00Z|         |    0|     0|         1728|                1871|          255779|           54248|                 1343519|d09d1f506dacacd07...|\n|20210301|    56|28399281|73334501|  11748|                46738|                869030|          9595|          44956|                 3171|                  4252|2021-03-01T24:00:00Z|504488|      869030|       355138357|2021-03-01T24:00:00Z|         |    0|     0|         1241|                1024|          118077|           48092|                 1154440|b8084497a1216c1ba...|\n|20210228|    56|28351189|73216424|  11708|                47352|                868006|          9802|          44907|                 3245|                  4252|2021-02-28T24:00:00Z|503247|      868006|       353983917|2021-02-28T24:00:00Z|         |    0|     0|         1051|                 879|          203599|           54349|                 1408422|c7cfdf3c2bf747296...|\n|20210227|    56|28296840|73012825|  11731|                48871|                867127|         10114|          44875|                 3335|                  4252|2021-02-27T24:00:00Z|502196|      867127|       352575495|2021-02-27T24:00:00Z|         |    0|     0|         1847|                1428|          205090|           71245|                 1655179|b1154e8002e47ec58...|\n|20210226|    56|28225595|72807735|  11945|                51112|                865699|         10466|          44791|                 3466|                  4247|2021-02-26T24:00:00Z|500349|      865699|       350920316|2021-02-26T24:00:00Z|         |    0|     0|         2141|                1933|          276829|           74857|                 1803309|24286d9bbff64d4ff...|\n+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Define fill values for nulls\nfill_values = {\n    \"date\": \"1970-01-01\",\n    \"states\": \"Unknown\",\n    \"positive\": 0,\n    \"negative\": 0,\n    \"pending\": 0,\n    \"hospitalizedCurrently\": 0,\n    \"hospitalizedCumulative\": 0,\n    \"inIcuCurrently\": 0,\n    \"inIcuCumulative\": 0,\n    \"onVentilatorCurrently\": 0,\n    \"onVentilatorCumulative\": 0,\n    \"dateChecked\": \"1970-01-01\",\n    \"death\": 0,\n    \"hospitalized\": 0,\n    \"totalTestResults\": 0,\n    \"lastModified\": \"1970-01-01\",\n    \"recovered\": 0,\n    \"total\": 0,\n    \"posNeg\": 0,\n    \"deathIncrease\": 0,\n    \"hospitalizedIncrease\": 0,\n    \"negativeIncrease\": 0,\n    \"positiveIncrease\": 0,\n    \"totalTestResultsIncrease\": 0,\n    \"hash\": \"Unknown\"\n}\n\n# Apply fill values for nulls\nsparkDf = sparkDf.na.fill(fill_values)\n# Showing the DataFrame to inspect the first 10 rows after filling values\nsparkDf.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\n|    date|states|positive|negative|pending|hospitalizedCurrently|hospitalizedCumulative|inIcuCurrently|inIcuCumulative|onVentilatorCurrently|onVentilatorCumulative|         dateChecked| death|hospitalized|totalTestResults|        lastModified|recovered|total|posNeg|deathIncrease|hospitalizedIncrease|negativeIncrease|positiveIncrease|totalTestResultsIncrease|                hash|\n+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\n|20210307|    56|28755524|74579770|  11808|                40212|                878613|          8137|          45475|                 2801|                  4281|2021-03-07T24:00:00Z|515142|      878613|       363789451|2021-03-07T24:00:00Z|         |    0|     0|          839|                 726|          130414|           41265|                 1156241|8b26839690cd05c0c...|\n|20210306|    56|28714259|74449356|  11783|                41401|                877887|          8409|          45453|                 2811|                  4280|2021-03-06T24:00:00Z|514303|      877887|       362633210|2021-03-06T24:00:00Z|         |    0|     0|         1674|                 503|          142201|           59620|                 1409138|d0c0482ea549c9d5c...|\n|20210305|    56|28654639|74307155|  12213|                42541|                877384|          8634|          45373|                 2889|                  4275|2021-03-05T24:00:00Z|512629|      877384|       361224072|2021-03-05T24:00:00Z|         |    0|     0|         2221|                2781|          271917|           68787|                 1744417|a35ea4289cec4bb55...|\n|20210304|    56|28585852|74035238|  12405|                44172|                874603|          8970|          45293|                 2973|                  4267|2021-03-04T24:00:00Z|510408|      874603|       359479655|2021-03-04T24:00:00Z|         |    0|     0|         1743|                1530|          177957|           65487|                 1590984|a19ad6379a653834c...|\n|20210303|    56|28520365|73857281|  11778|                45462|                873073|          9359|          45214|                 3094|                  4260|2021-03-03T24:00:00Z|508665|      873073|       357888671|2021-03-03T24:00:00Z|         |    0|     0|         2449|                2172|          267001|           66836|                 1406795|9e1d2afda1b0ec243...|\n|20210302|    56|28453529|73590280|  11196|                46388|                870901|          9465|          45084|                 3169|                  4257|2021-03-02T24:00:00Z|506216|      870901|       356481876|2021-03-02T24:00:00Z|         |    0|     0|         1728|                1871|          255779|           54248|                 1343519|d09d1f506dacacd07...|\n|20210301|    56|28399281|73334501|  11748|                46738|                869030|          9595|          44956|                 3171|                  4252|2021-03-01T24:00:00Z|504488|      869030|       355138357|2021-03-01T24:00:00Z|         |    0|     0|         1241|                1024|          118077|           48092|                 1154440|b8084497a1216c1ba...|\n|20210228|    56|28351189|73216424|  11708|                47352|                868006|          9802|          44907|                 3245|                  4252|2021-02-28T24:00:00Z|503247|      868006|       353983917|2021-02-28T24:00:00Z|         |    0|     0|         1051|                 879|          203599|           54349|                 1408422|c7cfdf3c2bf747296...|\n|20210227|    56|28296840|73012825|  11731|                48871|                867127|         10114|          44875|                 3335|                  4252|2021-02-27T24:00:00Z|502196|      867127|       352575495|2021-02-27T24:00:00Z|         |    0|     0|         1847|                1428|          205090|           71245|                 1655179|b1154e8002e47ec58...|\n|20210226|    56|28225595|72807735|  11945|                51112|                865699|         10466|          44791|                 3466|                  4247|2021-02-26T24:00:00Z|500349|      865699|       350920316|2021-02-26T24:00:00Z|         |    0|     0|         2141|                1933|          276829|           74857|                 1803309|24286d9bbff64d4ff...|\n+--------+------+--------+--------+-------+---------------------+----------------------+--------------+---------------+---------------------+----------------------+--------------------+------+------------+----------------+--------------------+---------+-----+------+-------------+--------------------+----------------+----------------+------------------------+--------------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Converting back to DynamicFrame for writing to S3\ncleaned_dynamic_frame = DynamicFrame.fromDF(sparkDf, glueContext, \"cleaned_dynamic_frame\")\n\n# Write the cleaned data back to S3\noutput_path = \"s3://etlemrbucket/cleandata/\"\nglueContext.write_dynamic_frame.from_options(\n    frame=cleaned_dynamic_frame,\n    connection_type=\"s3\",\n    connection_options={\"path\": output_path},\n    format=\"csv\"\n)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f83d62e2f80>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}